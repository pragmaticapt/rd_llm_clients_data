{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a53c558-1350-45f2-b419-e8a771ccfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain -q\n",
    "!pip install openai -q\n",
    "!pip install langchain_experimental -q\n",
    "!pip install langchain_openai -q\n",
    "!pip install tabulate -q\n",
    "!pip install google-search-results -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86fa978-3350-46db-8920-15a3a2ce8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from langchain import HuggingFaceHub, ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import ( \n",
    "    AgentExecutor, \n",
    "    create_openai_functions_agent,\n",
    "    load_tools, \n",
    "    initialize_agent, \n",
    "    AgentType\n",
    ")\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.agents import (\n",
    "    create_pandas_dataframe_agent, \n",
    "    create_csv_agent\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from mods_pcdiga.gcp.sales import get_df_sales_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cc4877-2a70-421c-8d91-7f85713bc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5acccfe4-6769-40d5-bc8e-029b2df417c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_json_keys = 'keys'\n",
    "os.environ['GCP_KEY_LAKE'] = os.path.join(dir_json_keys, 'data-lake-prod-385815-86d488f16f99.json')\n",
    "os.environ['GCP_KEY_LEGACY'] = os.path.join(dir_json_keys, 'ordinal-torch-248315-63cdd29d18ba.json')\n",
    "os.environ['GCP_KEY_AI'] = os.path.join(dir_json_keys, 'pcdiga-recommendation-ai-v2-045426a7004a.json')\n",
    "os.environ['GCP_KEY_REPLENISH'] = os.path.join(dir_json_keys, 'internal-replenishment-42a7bf181131.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c474e5e-30af-4194-8f97-af5a8870543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_df_sales_per_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d96c30-b59c-48c0-a4f1-45dd3a43f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('invoices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "801d315a-8dc5-4c5a-bdc6-d9ba9245dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('invoices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc1f89ab-e7fb-4d64-9f39-4b7a3376d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! openai migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bdafa89b-1bd4-40ba-a347-eae0a2c605e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# llm\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm = OpenAI(model_name=\"babbage-002\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm = OpenAI(model_name=\"text-moderation-007\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm = OpenAI(temperature=0.5, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "pd_agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "# pd_agent.run(\"Which SKU are most promissing to sell this week?\")\n",
    "\n",
    "# print(llm(\"Come up with a rap name for Matt Nikonorov\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79290e97-0c23-4b52-9bdb-343d25f48791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model = HuggingFaceHub(\n",
    "    repo_id=\"facebook/bart-large-cnn\",\n",
    "    model_kwargs={\"temperature\":0, \"max_length\":180}\n",
    ")\n",
    "\n",
    "def summarize(llm, text) -> str:\n",
    "    return llm(f\"Summarize this: {text}!\")\n",
    "\n",
    "def teach(llm, text) -> str:\n",
    "    return llm(f\"Teach me about this: {text}!\")\n",
    "\n",
    "# pandas agent\n",
    "pd_agent = create_pandas_dataframe_agent(model, df, verbose=True)\n",
    "# pd_agent.run(\"Which SKU are most promissing to sell this week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c63c66e4-9d2a-493b-8144-86fcea15095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "final_prompt = prompt.format(location='Paris')\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")\n",
    "\n",
    "# llmchain one\n",
    "template = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=template\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# llmchain two\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"city\"],\n",
    "    template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# llmchain one and llmchain two combined\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "final_answer = overall_chain.run(\"Canada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421cff5-523b-4aa1-b7c8-5f1f9a63513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two prompt llm chain\n",
    "\n",
    "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"],temperature=0),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n",
    "\n",
    "# Predict a sentence using the chatgpt chain\n",
    "output = chatgpt_chain.predict(\n",
    "    human_input=\"What is SingleStore?\"\n",
    ")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90890e-5ff5-4008-9efe-3740d8fe4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a friendly, informal assistant\"),\n",
    "    HumanMessage(content=\"Convince me that Djokovic is better than Federer\")\n",
    "]\n",
    "print(chat(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfeda4-bee5-4214-beae-4595fe43a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no agent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "print(llm(\"Come up with a rap name for Matt Nikonorov\"))\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "print(llm(\"Tell me a joke about data scientist\"))\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", huggingfacehub_api_token = os.environ[\"HUGGINGFACEHUB_API_KEY\"])\n",
    "print(llm(\"Tell me a joke about data scientist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a489859c-180f-454a-b474-72e7b8664b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"How much energy did wind turbines produce worldwide in 2022?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66a477f0-6711-446c-871a-884eb3dfee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai agent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59ea188d-1ace-4dab-9ee2-598190bb4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv agent\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = create_csv_agent(\n",
    "    llm,\n",
    "    \"invoices.csv\",\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")\n",
    "agent.run(\"In how many movies was Christian Bale casted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10b3bd8-bcc8-4e0d-adc6-3c7567711053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas agent\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "pd_agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "pd_agent.run(\"Which SKU are most promissing to sell this week?\")\n",
    "pd_agent.run(\"Which SKU are most promissing to sell this week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae51fa6-62b9-4be3-b56b-c7e2af9c1819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e813dcd2-09d5-420c-97fe-9833487bbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip install google-cloud-aiplatform>=1.38.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f0e922b8-48b4-4596-938a-921fe32c5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6270c499-8188-4e8c-8fd9-d659d9fab447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.0.53 requires langchain<0.2.0,>=0.1.8, but you have langchain 0.0.174 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers accelerate langchain==0.0.174 xformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dca638e8-4d85-47e3-a0e8-a2e56e208bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import cuda, bfloat16\n",
    "# import transformers\n",
    "# \n",
    "# device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e5e95a2-e6d2-4291-9681-2df6d337487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     'openlm-research/open_llama_3b_v2'\n",
    "# )\n",
    "# model.eval()\n",
    "# model.to(device)\n",
    "# print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b92d57a-f258-4326-8daa-88b6f25d0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#     'openlm-research/open_llama_3b_v2', use_fast=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fff0628b-2ce3-40e3-8f45-b2d8f3da16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_tokens_to_ids([''])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
